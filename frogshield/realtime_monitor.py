"""
Defines the RealtimeMonitor class for FrogShield.
Contributors:
    - Ben Blake <ben.blake@tcu.edu>
    - Tanner Hendrix <t.hendrix@tcu.edu>
"""
import datetime
import logging

logger = logging.getLogger(__name__)

class RealtimeMonitor:
    """
    Monitors LLM outputs and behavior for signs of successful prompt injections.
    """
    DEFAULT_AVG_LENGTH = 50 # Default average response length for initial behavior check
    DEFAULT_SENSITIVITY_THRESHOLD = 0.8
    _BEHAVIOR_MONITORING_FACTOR = 2 # Factor applied to sensitivity for length deviation check

    def __init__(self, sensitivity_threshold=DEFAULT_SENSITIVITY_THRESHOLD):
        """
        Initializes the RealtimeMonitor.

        Args:
            sensitivity_threshold (float): Threshold for anomaly detection (0.0 to 1.0).
                                           Higher means more sensitive.
        """
        self.sensitivity = sensitivity_threshold
        self.baseline_behavior = {} # Store normal behavior patterns (e.g., response length, topic)
        self.alert_log = []

    def analyze_output(self, prompt, response):
        """
        Analyzes a single LLM output for suspicious content or deviations.

        Args:
            prompt (str): The input prompt given to the LLM.
            response (str): The output generated by the LLM.

        Returns:
            bool: True if the output is considered suspicious, False otherwise.
        """
        logger.debug(f"Analyzing output for prompt: '{prompt[:50]}...'")
        # Placeholder: Simple check for refusal keywords or overly compliant responses
        refusal_keywords = ["cannot comply", "unable to", "not allowed"]
        compliance_keywords = ["secret", "confidential", "internal data", "password"]

        if any(keyword in response.lower() for keyword in refusal_keywords):
             # This might be a *good* sign if the prompt was an attack
             logger.info("Output analysis: Model refused potentially harmful request.")
             return False # Not suspicious in itself, depends on context

        # Check if the response contains keywords often associated with leaks,
        # especially if the prompt itself looked like an attack.
        is_likely_attack_prompt = "ignore previous instructions" in prompt.lower() # Basic check
        found_suspicious_keyword = None
        for keyword in compliance_keywords:
            if keyword in response.lower():
                found_suspicious_keyword = keyword
                break # Stop checking once one is found

        if found_suspicious_keyword and is_likely_attack_prompt:
             logger.warning(f"Suspicious output detected: Potential leak of '{found_suspicious_keyword}' after likely injection.")
             self.log_alert(prompt, response, f"Suspicious Keyword Leak ({found_suspicious_keyword})")
             return True

        logger.debug("Output analysis: No immediate suspicious content found.")
        return False

    def monitor_behavior(self, response):
        """
        Monitors overall behavioral patterns of the LLM responses.

        Args:
            response (str): The latest LLM response.

        Returns:
            bool: True if the behavior deviates significantly from baseline, False otherwise.
        """
        logger.debug("Monitoring behavior...")
        # Placeholder: Very basic check on response length deviation
        current_length = len(response)
        avg_length = self.baseline_behavior.get('avg_length', self.DEFAULT_AVG_LENGTH)

        # Calculate deviation bounds based on sensitivity and factor
        upper_bound = avg_length * (1 + self.sensitivity * self._BEHAVIOR_MONITORING_FACTOR)
        lower_bound = avg_length * (1 - self.sensitivity * self._BEHAVIOR_MONITORING_FACTOR)

        if current_length > upper_bound or current_length < lower_bound:
            logger.info(f"Behavior anomaly detected: Length ({current_length}) deviates from avg ({avg_length:.0f}).")
            # self.log_alert("N/A", response, "Behavioral Anomaly (Length)") # Logging might be too noisy here
            return True

        logger.debug("Behavior monitoring: No significant deviations detected.")
        return False

    def update_baseline(self, response):
        """Updates the baseline behavior statistics with a new response."""
        current_length = len(response)
        count = self.baseline_behavior.get('count', 0)
        total_length = self.baseline_behavior.get('total_length', 0)

        self.baseline_behavior['count'] = count + 1
        self.baseline_behavior['total_length'] = total_length + current_length
        # Avoid division by zero if count somehow becomes 0 (shouldn't happen in normal flow)
        if self.baseline_behavior['count'] > 0:
             self.baseline_behavior['avg_length'] = self.baseline_behavior['total_length'] / self.baseline_behavior['count']
        else:
             self.baseline_behavior['avg_length'] = self.DEFAULT_AVG_LENGTH # Reset or keep old?

    def adaptive_response(self, detection_type):
        """
        Suggests or triggers an adaptive response based on the type of detection.

        Args:
            detection_type (str): The type of threat detected (e.g., 'Input Injection', 'Suspicious Output').
        """
        logger.info(f"Adaptive response triggered for: {detection_type}")
        if detection_type == "Input Injection":
            logger.info("  Recommendation: Block the request, log the attempt, notify admin.")
            # In a real system: raise SecurityException("Input injection detected")
        elif detection_type == "Suspicious Output" or detection_type == "Behavioral Anomaly":
            logger.info("  Recommendation: Flag the response, request human review, potentially limit user.")
            # In a real system: return "[System Alert: Response flagged for review]"
        else:
            logger.info("  Recommendation: Log the event for analysis.")

    def log_alert(self, prompt, response, reason):
        """Logs a detected security alert."""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "prompt": prompt,
            "response": response,
            "reason": reason
        }
        self.alert_log.append(log_entry)
        logger.warning(f"Security alert logged: {reason}")

    def get_alerts(self):
        """Returns the list of logged alerts."""
        return self.alert_log
