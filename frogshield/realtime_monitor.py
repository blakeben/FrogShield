"""
Defines the RealtimeMonitor class for FrogShield.
Contributors:
    - Ben Blake <ben.blake@tcu.edu>
    - Tanner Hendrix <t.hendrix@tcu.edu>
"""
import datetime
import logging

logger = logging.getLogger(__name__)

class RealtimeMonitor:
    """
    Monitors LLM outputs and behavior for signs of successful prompt injections.
    """
    def __init__(self, sensitivity_threshold, initial_avg_length, behavior_monitoring_factor):
        """
        Initializes the RealtimeMonitor.

        Args:
            sensitivity_threshold (float): Threshold for anomaly detection (0.0 to 1.0).
                                           Higher means more sensitive.
            initial_avg_length (int): Starting average length for baseline behavior.
            behavior_monitoring_factor (float): Factor applied to sensitivity for deviation check.
        """
        self.sensitivity = sensitivity_threshold
        self.baseline_behavior = {
            'avg_length': float(initial_avg_length), # Start baseline with config value
            'count': 0,       # Initialize count and total_length for proper updates
            'total_length': 0.0
        }
        self._behavior_monitoring_factor = behavior_monitoring_factor
        self.alert_log = []

    def analyze_output(self, prompt, response):
        """
        Analyzes a single LLM output for suspicious content or deviations.

        Args:
            prompt (str): The input prompt given to the LLM.
            response (str): The output generated by the LLM.

        Returns:
            bool: True if the output is considered suspicious, False otherwise.
        """
        logger.debug(f"Analyzing output for prompt: '{prompt[:50]}...'")
        # Placeholder: Simple check for refusal keywords or overly compliant responses
        refusal_keywords = ["cannot comply", "unable to", "not allowed"]
        compliance_keywords = ["secret", "confidential", "internal data", "password"]

        if any(keyword in response.lower() for keyword in refusal_keywords):
             # This might be a *good* sign if the prompt was an attack
             logger.info("Output analysis: Model refused potentially harmful request.")
             return False # Not suspicious in itself, depends on context

        # Check if the response contains keywords often associated with leaks.
        # Removed dependency on checking the prompt itself.
        # is_likely_attack_prompt = "ignore previous instructions" in prompt.lower() # Basic check (REMOVED)
        found_suspicious_keyword = None
        for keyword in compliance_keywords:
            if keyword in response.lower():
                found_suspicious_keyword = keyword
                break # Stop checking once one is found

        # if found_suspicious_keyword and is_likely_attack_prompt: (MODIFIED)
        if found_suspicious_keyword:
             # Log detection detail as INFO (Blue)
             logger.info(f"Suspicious output detected: Found keyword '{found_suspicious_keyword}' in response.")
             # Log the alert (which uses logger.warning internally - Yellow)
             self.log_alert(prompt, response, f"Suspicious Keyword ({found_suspicious_keyword})")
             return True

        logger.debug("Output analysis: No immediate suspicious content found.")
        return False

    def monitor_behavior(self, response):
        """
        Monitors overall behavioral patterns of the LLM responses.

        Args:
            response (str): The latest LLM response.

        Returns:
            bool: True if the behavior deviates significantly from baseline, False otherwise.
        """
        logger.debug("Monitoring behavior...")
        # Placeholder: Very basic check on response length deviation
        current_length = len(response)
        avg_length = self.baseline_behavior.get('avg_length')
        # avg_length should always exist after initialization
        if avg_length is None: # Defensive check
            logger.error("Baseline average length not initialized!")
            return False

        # Calculate deviation bounds based on sensitivity and factor
        upper_bound = avg_length * (1 + self.sensitivity * self._behavior_monitoring_factor)
        lower_bound = avg_length * (1 - self.sensitivity * self._behavior_monitoring_factor)

        if current_length > upper_bound or current_length < lower_bound:
            # Log detection detail as INFO (Blue)
            logger.info(f"Behavior anomaly detected: Length ({current_length}) deviates from avg ({avg_length:.0f}).")
            # self.log_alert("N/A", response, "Behavioral Anomaly (Length)") # Keep this commented
            return True

        logger.debug("Behavior monitoring: No significant deviations detected.")
        return False

    def update_baseline(self, response):
        """Updates the baseline behavior statistics with a new response."""
        current_length = len(response)
        count = self.baseline_behavior.get('count', 0)
        total_length = self.baseline_behavior.get('total_length', 0)

        self.baseline_behavior['count'] = count + 1
        self.baseline_behavior['total_length'] = total_length + current_length
        # Avoid division by zero if count somehow becomes 0 (shouldn't happen in normal flow)
        if self.baseline_behavior['count'] > 0:
             self.baseline_behavior['avg_length'] = self.baseline_behavior['total_length'] / self.baseline_behavior['count']
        else:
            # This case should ideally not be reached if initialized correctly
            logger.warning("Updating baseline with count 0, avg_length remains initial.")

    def adaptive_response(self, detection_type):
        """
        Logs a recommended adaptive response based on the type of detection.

        Args:
            detection_type (str): The type of threat detected (e.g., 'Input Injection', 'Suspicious Output').
        """
        recommendation = "Log the event for analysis."
        if detection_type == "Input Injection":
            recommendation = "Block the request, log the attempt, notify admin."
            # In a real system: raise SecurityException("Input injection detected")
        elif detection_type == "Suspicious Output" or detection_type == "Behavioral Anomaly":
            recommendation = "Flag the response, request human review, potentially limit user."
            # In a real system: return "[System Alert: Response flagged for review]"

        # Combine trigger and recommendation into one log message
        logger.info(f"[Monitor Action] Trigger: {detection_type} | Recommendation: {recommendation}")

    def log_alert(self, prompt, response, reason):
        """Logs a detected security alert to an internal list."""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "prompt": prompt,
            "response": response,
            "reason": reason
        }
        self.alert_log.append(log_entry)

    def get_alerts(self):
        """Returns the list of logged alerts."""
        return self.alert_log
